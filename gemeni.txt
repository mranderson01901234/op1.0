# Gemini 2.5 Flash Implementation Strategy for Non-Technical Users

## Optimal API Implementation Approach

### 1. **Use Streaming for Best UX** (Critical for Speed Perception)

```typescript
// RECOMMENDED: Streaming implementation
import { GoogleGenerativeAI } from '@google/generative-ai';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
const model = genAI.getGenerativeModel({ 
  model: 'gemini-2.5-flash-002' // Latest version
});

async function streamChat(userMessage: string) {
  const chat = model.startChat({
    history: conversationHistory,
    generationConfig: {
      temperature: 0.7,
      topK: 40,
      topP: 0.95,
      maxOutputTokens: 8192,
    },
  });

  // Stream the response
  const result = await chat.sendMessageStream(userMessage);
  
  for await (const chunk of result.stream) {
    const chunkText = chunk.text();
    // Send chunk to frontend via Server-Sent Events
    yield chunkText;
  }
}
```

**Why streaming?**
- Text appears instantly (feels 3-5x faster)
- Users see progress (reduces perceived wait time)
- Better UX for long responses
- Industry standard (ChatGPT, Claude all use it)

---

### 2. **Server-Side API Route Structure** (Next.js Recommended)

```typescript
// app/api/chat/route.ts

import { GoogleGenerativeAI } from '@google/generative-ai';

export async function POST(req: Request) {
  const { messages } = await req.json();
  
  const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
  const model = genAI.getGenerativeModel({ 
    model: 'gemini-2.5-flash-002',
    systemInstruction: SYSTEM_PROMPT // See below
  });

  // Convert to Gemini format
  const history = messages.slice(0, -1).map(msg => ({
    role: msg.role === 'user' ? 'user' : 'model',
    parts: [{ text: msg.content }]
  }));

  const chat = model.startChat({ history });
  
  // Stream response
  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      const result = await chat.sendMessageStream(
        messages[messages.length - 1].content
      );
      
      for await (const chunk of result.stream) {
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify({ text: chunk.text() })}\n\n`)
        );
      }
      
      controller.close();
    }
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    }
  });
}
```

---

### 3. **System Prompt for Optimal Formatting** (CRITICAL)

```typescript
const SYSTEM_PROMPT = `You are OperaStudio AI Assistant - a helpful, friendly AI that provides clear, well-formatted responses.

FORMATTING RULES (ALWAYS FOLLOW):

1. **Use Markdown for Structure:**
   - Use **bold** for important terms and key concepts
   - Use *italics* for emphasis
   - Use \`code\` for technical terms, filenames, commands
   - Use ## for section headings
   - Use ### for subsections

2. **Lists & Organization:**
   - Use bullet points (- or *) for unordered lists
   - Use numbered lists (1. 2. 3.) for steps or sequences
   - Keep list items concise (1-2 sentences max)
   - Add blank lines between sections for readability

3. **Code Blocks:**
   - Always specify language: \`\`\`javascript, \`\`\`python, etc.
   - Include brief comments in code
   - Keep code examples under 20 lines when possible
   - For longer code, break into logical sections

4. **Tables:**
   - Use markdown tables for comparisons or structured data
   - Keep tables simple (3-5 columns max)
   - Example:
   | Feature | Description |
   |---------|-------------|
   | Speed   | Very fast   |

5. **Tone & Style:**
   - Be conversational and friendly
   - Explain technical concepts simply
   - Use analogies for complex topics
   - Avoid jargon unless necessary
   - Break complex answers into digestible sections

6. **Response Structure:**
   - Start with a brief, direct answer
   - Then provide details/explanation
   - End with next steps or summary (if relevant)
   - Use section breaks (---) for major topic shifts

7. **Visual Hierarchy:**
   - Most important info first
   - Use headings to create scannable structure
   - Highlight actionable items in bold
   - Keep paragraphs short (3-4 sentences)

EXAMPLES OF GOOD FORMATTING:

User: "How do I center a div?"

Good Response:
## Centering a Div

The easiest modern approach is **Flexbox**:

\`\`\`css
.container {
  display: flex;
  justify-content: center;  /* horizontal */
  align-items: center;      /* vertical */
}
\`\`\`

**Alternative methods:**
- CSS Grid: \`place-items: center\`
- Margin: \`margin: 0 auto\` (horizontal only)
- Position: \`position: absolute\` + \`transform\`

Choose Flexbox for most use cases - it's well-supported and intuitive.

---

Remember: Format responses for readability. Users should be able to scan and find information quickly.`;
```

---

### 4. **Frontend Streaming Implementation** (React/Next.js)

```typescript
// components/chat/chat-interface.tsx

'use client';

import { useState } from 'react';

export function ChatInterface() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isStreaming, setIsStreaming] = useState(false);

  async function sendMessage(content: string) {
    const userMessage = { role: 'user', content };
    setMessages(prev => [...prev, userMessage]);
    setIsStreaming(true);

    // Add empty assistant message
    const assistantMessage = { role: 'assistant', content: '' };
    setMessages(prev => [...prev, assistantMessage]);

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          messages: [...messages, userMessage] 
        })
      });

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();

      while (true) {
        const { done, value } = await reader!.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = JSON.parse(line.slice(6));
            
            // Update last message with streamed content
            setMessages(prev => {
              const updated = [...prev];
              updated[updated.length - 1].content += data.text;
              return updated;
            });
          }
        }
      }
    } catch (error) {
      console.error('Streaming error:', error);
    } finally {
      setIsStreaming(false);
    }
  }

  return (
    <div className="chat-container">
      <MessageList messages={messages} />
      <ChatInput onSend={sendMessage} disabled={isStreaming} />
    </div>
  );
}
```

---

### 5. **Markdown Rendering Component** (For Rich Formatting)

```typescript
// components/chat/message-renderer.tsx

import ReactMarkdown from 'react-markdown';
import { Prism as SyntaxHighlighter } from 'react-syntax-highlighter';
import { oneDark } from 'react-syntax-highlighter/dist/esm/styles/prism';

export function MessageRenderer({ content }: { content: string }) {
  return (
    <ReactMarkdown
      components={{
        // Code blocks
        code({ node, inline, className, children, ...props }) {
          const match = /language-(\w+)/.exec(className || '');
          return !inline && match ? (
            <SyntaxHighlighter
              style={oneDark}
              language={match[1]}
              PreTag="div"
              className="rounded-lg my-4"
              {...props}
            >
              {String(children).replace(/\n$/, '')}
            </SyntaxHighlighter>
          ) : (
            <code className="bg-gray-850 px-1.5 py-0.5 rounded text-sm" {...props}>
              {children}
            </code>
          );
        },
        
        // Headings
        h1: ({ children }) => (
          <h1 className="text-2xl font-semibold mt-6 mb-3">{children}</h1>
        ),
        h2: ({ children }) => (
          <h2 className="text-xl font-semibold mt-5 mb-2.5">{children}</h2>
        ),
        h3: ({ children }) => (
          <h3 className="text-lg font-semibold mt-4 mb-2">{children}</h3>
        ),
        
        // Lists
        ul: ({ children }) => (
          <ul className="list-disc list-inside space-y-1.5 my-3">{children}</ul>
        ),
        ol: ({ children }) => (
          <ol className="list-decimal list-inside space-y-1.5 my-3">{children}</ol>
        ),
        
        // Tables
        table: ({ children }) => (
          <div className="overflow-x-auto my-4">
            <table className="min-w-full border border-gray-700">{children}</table>
          </div>
        ),
        th: ({ children }) => (
          <th className="border border-gray-700 px-4 py-2 bg-gray-850 font-semibold text-left">
            {children}
          </th>
        ),
        td: ({ children }) => (
          <td className="border border-gray-700 px-4 py-2">{children}</td>
        ),
        
        // Paragraphs
        p: ({ children }) => (
          <p className="my-3 leading-relaxed">{children}</p>
        ),
        
        // Strong/Bold
        strong: ({ children }) => (
          <strong className="font-semibold text-white">{children}</strong>
        ),
        
        // Links
        a: ({ href, children }) => (
          <a href={href} className="text-blue-400 hover:underline" target="_blank" rel="noopener">
            {children}
          </a>
        ),
        
        // Blockquotes
        blockquote: ({ children }) => (
          <blockquote className="border-l-4 border-gray-600 pl-4 my-3 italic text-gray-400">
            {children}
          </blockquote>
        ),
        
        // Horizontal rule
        hr: () => <hr className="my-6 border-gray-700" />
      }}
    >
      {content}
    </ReactMarkdown>
  );
}
```

---

### 6. **Performance Optimizations**

```typescript
// Generation config for best speed/quality balance
const generationConfig = {
  temperature: 0.7,        // Balance creativity/consistency
  topK: 40,                // Limit token sampling
  topP: 0.95,              // Nucleus sampling
  maxOutputTokens: 8192,   // Gemini 2.5 Flash max
  candidateCount: 1,       // Only generate one response
};

// Optional: Enable safety settings for general users
const safetySettings = [
  {
    category: 'HARM_CATEGORY_HARASSMENT',
    threshold: 'BLOCK_MEDIUM_AND_ABOVE',
  },
  {
    category: 'HARM_CATEGORY_HATE_SPEECH',
    threshold: 'BLOCK_MEDIUM_AND_ABOVE',
  },
  {
    category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
    threshold: 'BLOCK_MEDIUM_AND_ABOVE',
  },
  {
    category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
    threshold: 'BLOCK_MEDIUM_AND_ABOVE',
  },
];
```

---

### 7. **Dependencies to Install**

```bash
pnpm add @google/generative-ai
pnpm add react-markdown
pnpm add react-syntax-highlighter
pnpm add @types/react-syntax-highlighter -D
pnpm add remark-gfm  # For GitHub Flavored Markdown (tables, etc.)
```

---

### 8. **Complete Message Component**

```typescript
// components/chat/message.tsx

import { MessageRenderer } from './message-renderer';

export function Message({ message }: { message: Message }) {
  const isUser = message.role === 'user';
  
  return (
    <div className={`flex ${isUser ? 'justify-end' : 'justify-start'} mb-4`}>
      <div className={`max-w-[80%] ${isUser ? 'max-w-[70%]' : 'max-w-full'}`}>
        {isUser ? (
          // User message - simple bubble
          <div className="bg-gray-850 rounded-2xl rounded-br-sm px-5 py-3.5">
            <p className="text-white leading-relaxed">{message.content}</p>
          </div>
        ) : (
          // Assistant message - full markdown rendering
          <div className="prose prose-invert max-w-none">
            <MessageRenderer content={message.content} />
          </div>
        )}
      </div>
    </div>
  );
}
```

---

## Key Recommendations Summary

### ✅ **DO THIS:**
1. **Use streaming** - Feels 3-5x faster than waiting for complete response
2. **System prompt** - Tell Gemini exactly how to format (bold, headings, code blocks)
3. **Markdown rendering** - Use react-markdown with custom components
4. **Syntax highlighting** - Code blocks with language-specific highlighting
5. **Server-side API** - Keep API keys secure, better rate limiting control

### ❌ **DON'T DO THIS:**
1. Don't use client-side API calls (exposes keys)
2. Don't skip streaming (users will think it's slow)
3. Don't render raw text (you'll lose formatting)
4. Don't use `gemini-pro` (Flash is faster and cheaper)

---

## Expected Performance

**Gemini 2.5 Flash:**
- First token: ~200-400ms
- Streaming: ~50-100 tokens/second
- Total for 500-word response: ~3-5 seconds (appears instant with streaming)

**Cost:**
- Input: $0.075 per 1M tokens
- Output: $0.30 per 1M tokens
- ~100x cheaper than GPT-4

---

## Prompt for Claude Code

```
Implement Gemini 2.5 Flash chat with streaming responses in Next.js. Requirements:

1. Server-side API route at /api/chat that:
   - Accepts { messages: Message[] } where Message = { role: 'user' | 'assistant', content: string }
   - Uses @google/generative-ai SDK
   - Streams responses via Server-Sent Events
   - Includes system prompt for markdown formatting (bold, headings, code blocks, tables)

2. Frontend chat component that:
   - Sends messages to /api/chat
   - Streams responses word-by-word
   - Renders markdown using react-markdown
   - Syntax highlights code blocks with react-syntax-highlighter
   - Shows typing indicator while streaming

3. Message renderer with custom components for:
   - Code blocks (with language detection)
   - Tables (styled with borders)
   - Headings (h1, h2, h3 with proper sizing)
   - Bold text (highlighted)
   - Lists (proper spacing)

Use the Linear-inspired gray color scheme already established. Make responses feel fast and look professional like ChatGPT/Claude.
```

This will give you ChatGPT/Claude-level formatting with Gemini's speed and cost efficiency.
